{
  "metadata": {
    "name": "movie_data_analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# HDFS에서 영화 데이터 다운로드"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\ndf1 \u003d spark.read \\\r\n           .option(\"header\", \"true\") \\\r\n           .option(\"inferSchema\", \"false\") \\\r\n           .csv(\"hdfs://spark-master-01:9000/skybluelee/movie/part-01.csv\") \r\n\r\ndf2 \u003d spark.read \\\r\n           .option(\"header\", \"true\") \\\r\n           .option(\"inferSchema\", \"false\") \\\r\n           .csv(\"hdfs://spark-master-01:9000/skybluelee/movie/part-02.csv\")\r\n            \r\ndf3 \u003d spark.read \\\r\n           .option(\"header\", \"true\") \\\r\n           .option(\"inferSchema\", \"false\") \\\r\n           .csv(\"hdfs://spark-master-01:9000/skybluelee/movie/part-03.csv\")\r\n            \r\ndf4 \u003d spark.read \\\r\n           .option(\"header\", \"true\") \\\r\n           .option(\"inferSchema\", \"false\") \\\r\n           .csv(\"hdfs://spark-master-01:9000/skybluelee/movie/part-04.csv\")\r\n            \r\ndf5 \u003d spark.read \\\r\n           .option(\"header\", \"true\") \\\r\n           .option(\"inferSchema\", \"false\") \\\r\n           .csv(\"hdfs://spark-master-01:9000/skybluelee/movie/part-05.csv\")\r\n            \r\ndf6 \u003d spark.read \\\r\n           .option(\"header\", \"true\") \\\r\n           .option(\"inferSchema\", \"false\") \\\r\n           .csv(\"hdfs://spark-master-01:9000/skybluelee/movie/part-06.csv\")            "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 데이터 타입 변경"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import IntegerType\n\ndf1 \u003d df1.withColumn(\u0027rating\u0027, col(\u0027rating\u0027).cast(IntegerType()))\\\n         .withColumn(\u0027spoiler_tag\u0027, col(\u0027spoiler_tag\u0027).cast(IntegerType()))\\\n         .withColumn(\"review_date\",to_timestamp(\"review_date\"))\n         \ndf2 \u003d df2.withColumn(\u0027rating\u0027, col(\u0027rating\u0027).cast(IntegerType()))\\\n         .withColumn(\u0027spoiler_tag\u0027, col(\u0027spoiler_tag\u0027).cast(IntegerType()))\\\n         .withColumn(\"review_date\",to_timestamp(\"review_date\"))\n         \ndf3 \u003d df3.withColumn(\u0027rating\u0027, col(\u0027rating\u0027).cast(IntegerType()))\\\n         .withColumn(\u0027spoiler_tag\u0027, col(\u0027spoiler_tag\u0027).cast(IntegerType()))\\\n         .withColumn(\"review_date\",to_timestamp(\"review_date\"))\n         \ndf4 \u003d df4.withColumn(\u0027rating\u0027, col(\u0027rating\u0027).cast(IntegerType()))\\\n         .withColumn(\u0027spoiler_tag\u0027, col(\u0027spoiler_tag\u0027).cast(IntegerType()))\\\n         .withColumn(\"review_date\",to_timestamp(\"review_date\"))\n         \ndf5 \u003d df5.withColumn(\u0027rating\u0027, col(\u0027rating\u0027).cast(IntegerType()))\\\n         .withColumn(\u0027spoiler_tag\u0027, col(\u0027spoiler_tag\u0027).cast(IntegerType()))\\\n         .withColumn(\"review_date\",to_timestamp(\"review_date\"))\n         \ndf6 \u003d df6.withColumn(\u0027rating\u0027, col(\u0027rating\u0027).cast(IntegerType()))\\\n         .withColumn(\u0027spoiler_tag\u0027, col(\u0027spoiler_tag\u0027).cast(IntegerType()))\\\n         .withColumn(\"review_date\",to_timestamp(\"review_date\"))         \n         \n           \ndf1.createOrReplaceTempView(\"part_01\")\ndf2.createOrReplaceTempView(\"part_02\")\ndf3.createOrReplaceTempView(\"part_03\")\ndf4.createOrReplaceTempView(\"part_04\")\ndf5.createOrReplaceTempView(\"part_05\")\ndf6.createOrReplaceTempView(\"part_06\")         "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf1.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 각 파일의 데이터 확인"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_year_check\u003d spark.sql(\"\"\"\n                            SELECT SUBSTR(review_date, 1, 4) AS Year, COUNT(*) AS review_count\n                            FROM   part_06\n                            GROUP  BY Year\n                            ORDER  BY 1 DESC\n                         \"\"\") \\\n                    .show(100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 2019~2021년에 해당하는 데이터를 해당 년도의 데이터만 존재하도록 설정"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_01 \u003d spark.sql(\"\"\"\n                     SELECT  *, YEAR(review_date) AS YEAR, MONTH(review_date) AS MONTH,\n                             CASE WHEN DAY(review_date) BETWEEN 1 AND 10 THEN \u00271_10\u0027\n                                  WHEN DAY(review_date) BETWEEN 11 AND 20 THEN \u002711_20\u0027\n                                  ELSE \u002721_31\u0027 END AS DAY\n                     FROM    part_01\n                     WHERE   YEAR(review_date) IN (2019, 2020, 2021)\n                 \"\"\")\n                      \ndf_02 \u003d spark.sql(\"\"\"\n                     SELECT  *, YEAR(review_date) AS YEAR, MONTH(review_date) AS MONTH,\n                             CASE WHEN DAY(review_date) BETWEEN 1 AND 10 THEN \u00271_10\u0027\n                                  WHEN DAY(review_date) BETWEEN 11 AND 20 THEN \u002711_20\u0027\n                                  ELSE \u002721_31\u0027 END AS DAY\n                     FROM    part_02\n                     WHERE   YEAR(review_date) IN (2019, 2020, 2021)\n                 \"\"\")\n                      \ndf_03 \u003d spark.sql(\"\"\"\n                     SELECT  *, YEAR(review_date) AS YEAR, MONTH(review_date) AS MONTH,\n                             CASE WHEN DAY(review_date) BETWEEN 1 AND 10 THEN \u00271_10\u0027\n                                  WHEN DAY(review_date) BETWEEN 11 AND 20 THEN \u002711_20\u0027\n                                  ELSE \u002721_31\u0027 END AS DAY\n                     FROM    part_03\n                     WHERE   YEAR(review_date) IN (2019, 2020, 2021)\n                 \"\"\")  \n                      \ndf_04 \u003d spark.sql(\"\"\"\n                     SELECT  *, YEAR(review_date) AS YEAR, MONTH(review_date) AS MONTH,\n                             CASE WHEN DAY(review_date) BETWEEN 1 AND 10 THEN \u00271_10\u0027\n                                  WHEN DAY(review_date) BETWEEN 11 AND 20 THEN \u002711_20\u0027\n                                  ELSE \u002721_31\u0027 END AS DAY\n                     FROM    part_04\n                     WHERE   YEAR(review_date) IN (2019, 2020, 2021)\n                 \"\"\")           \n\ndf_total \u003d df_01                    \ndf_total \u003d df_total.union(df_02) \ndf_total \u003d df_total.union(df_03) \ndf_total \u003d df_total.union(df_04)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 해당 SQL 결과"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_01 \u003d spark.sql(\"\"\"\n                     SELECT  *, YEAR(review_date) AS YEAR, MONTH(review_date) AS MONTH,\n                             CASE WHEN DAY(review_date) BETWEEN 1 AND 10 THEN \u00271_10\u0027\n                                  WHEN DAY(review_date) BETWEEN 11 AND 20 THEN \u002711_20\u0027\n                                  ELSE \u002721_31\u0027 END AS DAY\n                     FROM    part_01\n                     WHERE   YEAR(review_date) IN (2019, 2020, 2021)\n                 \"\"\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 데이터를 파티셔닝하여 HDFS에 업로드"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_total.coalesce(1).write \\\n                    .option(\"header\",True) \\\n                    .partitionBy(\"YEAR\", \"MONTH\", \"DAY\") \\\n                    .mode(\"overwrite\") \\\n                    .csv(\"hdfs://spark-master-01:9000/skybluelee/movie-partitioned\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}